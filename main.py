import re
import torch
import unicodedata
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from transformers import MT5Tokenizer, MT5ForConditionalGeneration
from difflib import SequenceMatcher



app = FastAPI()

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for now
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Load the model directly from Hugging Face
model_path = "RanawahajAhmed/finetuned_mt5_large_for_urdu_correction"  # Replace with your username
tokenizer = MT5Tokenizer.from_pretrained(model_path, use_fast=False)
model = MT5ForConditionalGeneration.from_pretrained(model_path)

# Error dictionaries (same as provided)
noun_errors = {
    "پاکستان": "پاکیستان", "لوگ": "لوک", "مسائل": "مسایل", "مہنگائی": "مہنگای", "حکومت": "حوکمت",
    "تعلیم": "تعلم", "صحت": "سحت", "گاؤں": "گاؤن", "موسم": "موسوم", "ٹیکنالوجی": "ٹکنالوجی",
    "پانی": "پانے", "کرپشن": "کرفشن", "روزگار": "روزگاز", "سیاحت": "سایاحت", "نوجوان": "نووجوان",
    "زراعت": "زرات", "صنعت": "سنعت", "سیلاب": "سیلاپ", "ٹرانسپورٹ": "ٹرانسپوٹ", "خواتین": "خواتیں",
    "کھیل": "کحیل", "آلودگی": "الودگی", "امید": "امد", "دوستی": "دوسٹی", "زندگی": "زندکی"
}

verb_errors = {
    "کرتے": "کرو", "بتا": "بولو", "بڑھ": "چڑھ", "ہو": "جا", "آتی": "جاتی", "رکھنا": "اٹھانا",
    "سوچنا": "سونگھنا", "ملتی": "ٹلتی", "آزماتے": "چکھتے", "نکلتی": "پھسلتی", "دیتا": "بیچتا",
    "بدل": "پھٹ", "رکھتی": "پکڑتی", "آتا": "بھاگتا", "چھوڑ": "پھوڑ", "لینا": "دھونا", "دیکھنا": "چومنا",
    "سمجھنا": "چلنا", "پڑھنا": "گھومنا", "چلنا": "ہنسنا", "لگایا": "جلایا", "کمایا": "سمجھایا",
    "کیا": "پیا", "دیا": "کیا", "کیں": "سیں", "آئیں": "جائیں", "دیں": "لیں", "کریں": "سریں"
}

pronoun_errors = {
    "وہ": "یہ", "ہم": "تم", "اس": "ان", "ان": "اس", "ہمارے": "تمہارے", "یہ": "وہ", "کسی": "کوئی",
    "جو": "جس", "ہر": "کوئی", "اپنی": "ان کی", "تم": "ہم", "میں": "ہم", "ہمیں": "تمہیں", "انہوں": "اس نے",
    "کس": "کیا", "کچھ": "سب", "کوئی": "ہر", "خود": "دوسرے", "ان کا": "ہمارا", "اپنوں": "دوسروں",
    "ہم سب": "تم سب", "ان سب": "اس سب", "کس نے": "کیا نے", "جس نے": "جو نے", "جو کچھ": "کچھ بھی",
    "ہر ایک": "کوئی ایک", "ان کے": "ہمارے", "ہم نے": "تم نے", "تم نے": "ہم نے"
}

prepositions_conjunctions_errors = {
    "میں": "سے", "سے": "میں", "کے": "پر", "پر": "کے", "اور": "یا", "تا کہ": "کیونکہ", "کی": "کا",
    "کہ": "جو", "لیے": "بغیر", "بھی": "نہ", "جس": "کہ", "اگر": "ورنہ", "تو": "مگر", "جب": "کہاں",
    "تک": "سے", "چونکہ": "تاکہ", "کیونکہ": "اگر", "جو": "کہ", "جن": "جس", "ورنہ": "اگر", "یا": "اور",
    "مگر": "تو", "بل کہ": "بلکہ", "جب کہ": "کیونکہ"
}

object_errors = {
    "مسائل": "مسایئل", "مہنگائی": "مہنگای", "تعلیم": "تلیم", "صحت": "سحت", "پانی": "پانے", "کرپشن": "کرپسشن",
    "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان", "زراعت": "زراعٹ", "صنعت": "سنعت",
    "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپوٹ", "خواتین": "خواتیں", "کھیل": "خیل", "آلودگی": "آلدگی",
    "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "اصلاحات": "اصلحات",
    "سرمایہ": "سرمائہ", "منصوبے": "منصوبہ", "حقوق": "حقووق", "وسائل": "وسائیل"
}

past_tense_verb_errors = {
    "بڑھ گئی": "بڑھتا", "ہو گیا": "ہوتا", "متاثر ہوئی": "متاثر ہوتا", "آ گئے": "آتا", "کہا": "کہتا",
    "بتایا": "بتاتا", "بنائے گئے": "بناتا", "دیا": "دیتا", "پڑھا": "پڑھتا", "چلا": "چلتا", "لگا": "لگتا",
    "ہوئے": "ہوتا", "رکھا": "رکھتا", "کیا": "کرتا", "آئی": "آتا", "دیکھا": "دیکھتا", "سمجھا": "سمجھتا",
    "چھوڑا": "چھوڑتا", "ہوئیں": "ہوتا", "پہنچا": "پہنچتا", "مل گیا": "ملتا", "بدل گیا": "بدلتا",
    "کمایا": "کماتا", "لگایا": "لگاتا", "بنا": "بناتا", "بڑھائی": "بڑھاتا", "پایا": "پاتا", "کیے": "کرتا",
    "سوچا": "سوچتا", "آزمایا": "آزماتا", "نکلا": "نکلتا", "بدلا": "بدلتا", "آیا": "آتا", "لیا": "لیتا",
    "رکھے": "رکھتا", "کیں": "کرتا", "آئیں": "آتا", "دیں": "دیتا", "بڑھائیں": "بڑھاتا", "پائیں": "پاتا",
    "کریں": "کرتا", "ہوئی": "ہوتا", "گئی": "جاتا"
}

present_tense_verb_errors = {
    "کرتے ہیں": "کریں گے", "بتا رہا ہے": "بتائے گا", "بڑھ رہی ہے": "بڑھے گی", "ہوتا ہے": "ہو گا",
    "لگتا ہے": "لگے گا", "چاہیے": "چاہیے گا", "دینی ہے": "دے گا", "متاثر ہو رہا ہے": "متاثر ہو گا",
    "پڑتا ہے": "پڑے گا", "حاصل کر رہے ہیں": "حاصل کریں گے", "دیتی ہے": "دے گی", "بناتے ہیں": "بنائیں گے",
    "آتی ہے": "آئے گی", "رکھتا ہے": "رکھے گا", "سوچتے ہیں": "سوچیں گے", "ملتی ہے": "ملے گی",
    "آزماتے ہیں": "آزمائیں گے", "نکلتی ہے": "نکلے گی", "دیتا ہے": "دے گا", "بدلتے ہیں": "بدلیں گے",
    "رکھتی ہے": "رکھے گی", "آتا ہے": "آئے گا", "چھوڑتے ہیں": "چھوڑیں گے", "لیتے ہیں": "لیں گے",
    "دیکھتے ہیں": "دیکھیں گے", "سمجھتے ہیں": "سمجھیں گے", "پڑھتے ہیں": "پڑھیں گے", "چلتے ہیں": "چلیں گے",
    "لگاتے ہیں": "لگائیں گے", "کماتے ہیں": "کمائیں گے", "رکھتے ہیں": "رکھیں گے", "کرتی ہے": "کرے گی",
    "بتاتی ہے": "بتائے گی", "بڑھتا ہے": "بڑھے گا", "لگتی ہے": "لگے گی", "دیتے ہیں": "دیں گے",
    "بناتی ہے": "بنائے گی", "آتے ہیں": "آئیں گے", "رکھتی ہیں": "رکھیں گی", "سوچتی ہے": "سوچے گی",
    "ملتا ہے": "ملے گا", "آزماتی ہے": "آزمائے گی", "نکلتے ہیں": "نکلیں گے", "بدلتی ہے": "بدلے گی",
    "چھوڑتی ہے": "چھوڑے گی", "لیتی ہے": "لے گی", "دیکھتی ہے": "دیکھے گی", "سمجھتی ہے": "سمجھے گی",
    "پڑھتی ہے": "پڑھے گی", "چلتی ہے": "چلے گی", "لگاتی ہے": "لگائے گی", "کماتی ہے": "کمائے گی",
    "کر رہے ہیں": "کریں گے", "بتا رہے ہیں": "بتائیں گے", "بڑھ رہے ہیں": "بڑھیں گے", "لگ رہے ہیں": "لگیں گے"
}

future_tense_verb_errors = {
    "ہو گا": "ہوا", "کر سکتے ہیں": "کر سکتے تھے", "آئے گا": "آیا", "دیں گے": "دیا", "بنا سکتے ہیں": "بنا سکتے تھے",
    "پائیں گے": "پایا", "ملے گا": "ملا", "ہوں گی": "تھیں", "کریں گے": "کیا", "بڑھے گا": "بڑھا",
    "رکھ سکتے ہیں": "رکھ سکتے تھے", "سوچ سکتے ہیں": "سوچ سکتے تھے", "بدل سکتے ہیں": "بدل سکتے تھے",
    "چھوڑیں گے": "چھوڑا", "لیں گے": "لیا", "آئیں گے": "آئے", "پڑھ سکتے ہیں": "پڑھ سکتے تھے",
    "دیکھیں گے": "دیکھا", "سمجھیں گے": "سمجھا", "ہو سکتے ہیں": "ہو سکتے تھے", "لگے گا": "لگا",
    "کمایا جائے گا": "کمایا گیا", "لگایا جائے گا": "لگایا گیا", "بنایا جائے گا": "بنایا گیا",
    "بڑھایا جائے گا": "بڑھایا گیا", "پایا جائے گا": "پایا گیا", "کیا جائے گا": "کیا گیا",
    "رکھا جائے گا": "رکھا گیا", "دیا جائے گا": "دیا گیا"
}

singular_errors = {
    "پاکستان": "پاکستن", "دوست": "دووست", "مسئلہ": "مسئیلہ", "مہنگائی": "مہنگای", "حکومت": "حکومٹ",
    "تعلیم": "تلیم", "صحت": "سحت", "گاؤں": "گاؤن", "موسم": "موسسم", "ٹیکنالوجی": "ٹکنالوجی",
    "پانی": "پانے", "کرپشن": "کرپسشن", "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان",
    "زراعت": "زراعٹ", "صنعت": "سنعت", "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خاتون": "خاتوون",
    "کھیل": "خیل", "آلودگی": "آلدگی", "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے"
}

plural_errors = {
    "لوگ": "لوک", "مسائل": "مسایئل", "اخراجات": "اخرجت", "سہولتیں": "سہولتین", "ادارے": "ادارہ",
    "طلبہ": "طلباء", "والدین": "والداین", "نوجوانوں": "نواجوانوں", "موسموں": "موسسموں", "مہارتیں": "مہارتین",
    "وسائل": "وسائیل", "مواقع": "مواقق", "اصلاحات": "اصلحات", "منصوبوں": "منصوبہ", "حقوق": "حقووق",
    "کھیلوں": "خیلوں", "شہروں": "شہرن", "بچوں": "بچن", "پروگرامز": "پروگرمز", "کاروباروں": "کارباروں",
    "سیاحوں": "سیاحن", "مریضوں": "مریضن", "پالیسیوں": "پالسیوں", "اتفاقیات": "اتفقیات", "یادیں": "یادین"
}

masculine_errors = {
    "دوست": "دووست", "شخص": "شخس", "نوجوان": "نواجوان", "طالب": "طلیب", "والد": "والدد", "کسان": "کسسن",
    "سیاح": "سیح", "مریض": "مرض", "ڈاکٹر": "ڈکٹر", "سیاستدان": "سیاسدان", "گاؤں": "گاؤن", "شہر": "شہہر",
    "موسم": "موسسم", "روزگار": "روزگارد", "کاروبار": "کاربار", "منصوبہ": "منصوبہہ", "کھیل": "خیل",
    "پانی": "پانے", "ماحول": "محول", "وسائل": "وسائیل", "امن": "امم", "صبر": "سبر", "خواب": "خوب",
    "رشتہ": "رشہ", "چیلنج": "چلنج"
}

feminine_errors = {
    "مہنگائی": "مہنگای", "حکومت": "حکومٹ", "تعلیم": "تلیم", "صحت": "سحت", "ٹیکنالوجی": "ٹکنالوجی",
    "کرپشن": "کرپسشن", "سیاحت": "سیاحٹ", "زراعت": "زراعٹ", "صنعت": "سنعت", "آلودگی": "آلدگی",
    "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولت": "سہوللت", "اصلاح": "اصلح",
    "سرمایہ": "سرمائہ", "پالیسی": "پالسی", "خاتون": "خاتوون", "مہارت": "محارت", "کوشش": "کوشس",
    "مسکراہٹ": "مسکاہٹ", "خاموشی": "خموشی", "خوشی": "خشی", "یاد": "یادد", "ترقی": "ترقے"
}

direct_object_errors = {
    "مسائل": "مسایئل", "مہنگائی": "مہنگای", "تعلیم": "تلیم", "صحت": "سحت", "پانی": "پانے", "کرپشن": "کرپسشن",
    "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان", "زراعت": "زراعٹ", "صنعت": "سنعت",
    "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خواتین": "خواتیں", "کھیل": "خیل", "آلودگی": "آلدگی",
    "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "اصلاحات": "اصلحات",
    "سرمایہ": "سرمائہ", "منصوبے": "منصوبہ", "حقوق": "حقووق", "وسائل": "وسائیل"
}

indirect_object_errors = {
    "پاکستان میں": "پاکستن میں", "گاؤں میں": "گاؤن میں", "شہر میں": "شہہر میں", "مارکیٹ میں": "مارکیٹٹ میں",
    "اسکولوں میں": "اسکولن میں", "اسپتالوں میں": "اسپتلن میں", "شعبوں میں": "شعبن میں", "میدانوں میں": "میدانن میں",
    "اداروں میں": "ادارن میں", "منصوبوں میں": "منصوبن میں", "موسموں میں": "موسسموں میں", "حالات میں": "حالت میں",
    "زندگی میں": "زندگے میں", "معاشرے میں": "معاشرہ میں", "دور میں": "دورر میں", "وقت میں": "وققت میں",
    "ماحول میں": "محول میں", "سہولتوں میں": "سہولتین میں", "مسائل میں": "مسایئل میں", "حقوق میں": "حقووق میں",
    "ترقی میں": "ترقے میں", "امن میں": "امم میں", "خوابوں میں": "خوابن میں", "امید میں": "امد میں",
    "دوستی میں": "دوستے میں"
}

adverb_errors = {
    "زیادہ": "زیادہہ", "فوری": "فورری", "سست": "سسٹ", "برابر": "برابار", "مناسب طور پر": "مناسب طورر پر",
    "شدید": "شدیدد", "آہستہ": "آہستہہ", "ہمیشہ": "ہمشہ", "کبھی": "کبھے", "ابھی": "ابھے", "پہلے": "پہلہ",
    "بعد": "بدد", "صرف": "سررف", "بہت": "بہہت", "کم": "ککم", "دوبارہ": "دوبارہہ", "اکثر": "اکسر",
    "ہر سال": "ہرر سال", "جلدی": "جلدے", "صحیح": "سحیح", "واضح": "واضضح", "خاموشی سے": "خموشی سے",
    "درست": "درسٹ", "سادہ": "سادہہ"
}

possessive_noun_errors = {
    "پاکستان کی": "پاکستن کی", "لوگوں کی": "لوکوں کی", "حکومت کی": "حکومٹ کی", "تعلیم کی": "تلیم کی",
    "صحت کی": "سحت کی", "گاؤں کی": "گاؤن کی", "موسم کی": "موسسم کی", "ٹیکنالوجی کی": "ٹکنالوجی کی",
    "پانی کی": "پانے کی", "کرپشن کی": "کرپسشن کی", "روزگار کی": "روزگارد کی", "سیاحت کی": "سیاحٹ کی",
    "نوجوانوں کی": "نواجوانوں کی", "زراعت کی": "زراعٹ کی", "صنعت کی": "سنعت کی", "سیلاب کی": "سیلب کی",
    "ٹرانسپورٹ کی": "ٹرانسپرٹ کی", "خواتین کی": "خواتیں کی", "کھیل کی": "خیل کی", "آلودگی کی": "آلدگی کی",
    "امید کی": "امد کی", "دوستی کی": "دوستے کی", "زندگی کی": "زندگے کی", "سہولت کی": "سہوللت کی",
    "اصلاح کی": "اصلح کی"
}

possessed_noun_errors = {
    "معیشت": "معشیت", "ترقی": "ترقے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "حالت": "حالتت",
    "پیداوار": "پیدوار", "سرمایہ": "سرمائہ", "پالیسی": "پالسی", "مہارت": "محارت", "کوشش": "کوشس",
    "مسکراہٹ": "مسکاہٹ", "خاموشی": "خموشی", "خوشی": "خشی", "یاد": "یادد", "امید": "امد", "دوستی": "دوستے",
    "سادگی": "سادگے", "سکون": "سکوون", "احساس": "احسس", "شخصیت": "شخسیت", "بنیاد": "بنیادد",
    "رفتاری": "رفتارے", "چیزوں": "چیزن", "اعتماد": "اعتمد"
}

sentence_component_errors = {
    "پاکستان": "پاکستن", "لوگ": "لوک", "کرتے": "کریت", "مسائل": "مسایئل", "مہنگائی": "مہنگای",
    "حکومت": "حکومٹ", "تعلیم": "تلیم", "صحت": "سحت", "گاؤں": "گاؤن", "موسم": "موسسم", "ٹیکنالوجی": "ٹکنالوجی",
    "پانی": "پانے", "کرپشن": "کرپسشن", "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان",
    "زراعت": "زراعٹ", "صنعت": "سنعت", "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خواتین": "خواتیں",
    "کھیل": "خیل", "آلودگی": "آلدگی", "امید": "امد", "دوستی": "دوستے"
}

subject_errors = {
    "لوگ": "لوک", "دوست": "دووست", "شخص": "شخس", "حکومت": "حکومٹ", "نوجوان": "نواجوان", "والدین": "والداین",
    "کسان": "کسسن", "سیاح": "سیح", "مریض": "مرض", "ڈاکٹر": "ڈکٹر", "سیاستدان": "سیاسدان", "طلبہ": "طلباء",
    "خواتین": "خواتیں", "پاکستان": "پاکستن", "گاؤں": "گاؤن", "شہر": "شہہر", "موسم": "موسسم",
    "ٹیکنالوجی": "ٹکنالوجی", "پانی": "پانے", "کرپشن": "کرپسشن", "روزگار": "روزگارد", "سیاحت": "سیاحٹ",
    "زراعت": "زراعٹ", "صنعت": "سنعت", "آلودگی": "آلدگی"
}

verb_object_errors = {
    "مسائل": "مسایئل", "مہنگائی": "مہنگای", "تعلیم": "تلیم", "صحت": "سحت", "پانی": "پانے", "کرپشن": "کرپسشن",
    "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان", "زراعت": "زراعٹ", "صنعت": "سنعت",
    "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خواتین": "خواتیں", "کھیل": "خیل", "آلودگی": "آلدگی",
    "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "اصلاحات": "اصلحات",
    "سرمایہ": "سرمائہ", "منصوبے": "منصوبہ", "حقوق": "حقووق", "وسائل": "وسائیل"
}

# Combine all dictionaries into a list for sequential processing
error_dictionaries = [
    ("noun_errors", noun_errors, "اسم کی غلطی", "یہ اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
    ("verb_errors", verb_errors, "فعل کی غلطی", "یہ فعل کی غلطی ہے کیونکہ یہ غلط زمانے یا شکل میں ہے۔"),
    ("pronoun_errors", pronoun_errors, "ضمیر کی غلطی", "یہ ضمیر کی غلطی ہے کیونکہ یہ غلط فاعل یا مفعول کی طرف اشارہ کرتا ہے۔"),
    ("prepositions_conjunctions_errors", prepositions_conjunctions_errors, "حرف ربط یا حرف جار کی غلطی", "یہ حرف ربط یا حرف جار کی غلطی ہے کیونکہ اسے جملے میں غلط استعمال کیا گیا ہے۔"),
    ("object_errors", object_errors, "مفعول کی غلطی", "یہ مفعول کی غلطی ہے کیونکہ مفعول کی ہجے غلط ہے یا غلط ہے۔"),
    ("past_tense_verb_errors", past_tense_verb_errors, "فعل ماضی کی غلطی", "یہ فعل ماضی کی غلطی ہے کیونکہ یہ غلط شکل میں ہے۔"),
    ("present_tense_verb_errors", present_tense_verb_errors, "فعل حال کی غلطی", "یہ فعل حال کی غلطی ہے کیونکہ یہ غلط شکل میں ہے۔"),
    ("future_tense_verb_errors", future_tense_verb_errors, "فعل مستقبل کی غلطی", "یہ فعل مستقبل کی غلطی ہے کیونکہ یہ غلط شکل میں ہے۔"),
    ("singular_errors", singular_errors, "واحد کی غلطی", "یہ واحد اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
    ("plural_errors", plural_errors, "جمع کی غلطی", "یہ جمع اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
    ("masculine_errors", masculine_errors, "مذکر کی غلطی", "یہ مذکر اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
    ("feminine_errors", feminine_errors, "مؤنث کی غلطی", "یہ مؤnث اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
    ("direct_object_errors", direct_object_errors, "مفعول مستقیم کی غلطی", "یہ مفعول مستقیم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
    ("indirect_object_errors", indirect_object_errors, "مفعول غیر مستقیم کی غلطی", "یہ مفعول غیر مستقیم کی غلطی ہے کیونکہ اسے غلط استعمال کیا گیا ہے۔"),
    ("adverb_errors", adverb_errors, "متعلق فعل کی غلطی", "یہ متعلق فعل کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
    ("possessive_noun_errors", possessive_noun_errors, "اسم ملکیتی کی غلطی", "یہ اسم ملکیتی کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
    ("possessed_noun_errors", possessed_noun_errors, "اسم مملوک کی غلطی", "یہ اسم مملوک کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
    ("sentence_component_errors", sentence_component_errors, "جملہ کے جزو کی غلطی", "یہ جملہ کے جزو کی غلطی ہے کیونکہ اسے غلط استعمال کیا گیا ہے۔"),
    ("subject_errors", subject_errors, "فاعل کی غلطی", "یہ فاعل کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
    ("verb_object_errors", verb_object_errors, "فعل اور مفعول کی غلطی", "یہ فعل اور مفعول کی غلطی ہے کیونکہ مفعول فعل کے ساتھ مطابقت نہیں رکھتا۔")
]



def detect_errors(input_text, corrected_text):
    """
    Detect errors by comparing input text with corrected text using sequence alignment.
    Identifies errors corrected by the model according to error dictionaries.
    Returns a list of unique detected errors with explanations and reasons in Urdu.
    """
    detected_errors = []
    seen_errors = set()  # To avoid duplicates

    # Normalize Unicode to handle encoding variations
    input_text = unicodedata.normalize('NFC', input_text)
    corrected_text = unicodedata.normalize('NFC', corrected_text)

    # Tokenize input and corrected text into words
    input_words = re.findall(r'\S+', input_text)
    corrected_words = re.findall(r'\S+', corrected_text)

    # Log tokenized words for debugging
    print("Input words:", input_words)
    print("Corrected words:", corrected_words)

    # Use SequenceMatcher to align input and corrected words
    matcher = SequenceMatcher(None, input_words, corrected_words)
    matches = matcher.get_opcodes()

    # Log alignment operations
    print("Alignment operations:", matches)

    # Process alignment operations
    for tag, i1, i2, j1, j2 in matches:
        if tag == 'equal':
            # Words match, no error
            continue
        elif tag == 'replace':
            # Words differ, check for errors
            for i in range(i1, i2):
                input_word = unicodedata.normalize('NFC', input_words[i])
                corrected_word = unicodedata.normalize('NFC', corrected_words[j1 + (i - i1)])
                print(f"Comparing replace at input[{i}]={input_word} -> corrected[{j1 + (i - i1)}]={corrected_word}")
                for dict_name, error_dict, error_type, reason in error_dictionaries:
                    if dict_name in ["indirect_object_errors", "possessive_noun_errors", "adverb_errors"]:
                        for correct_word, incorrect_word in error_dict.items():
                            if input_word == incorrect_word and corrected_word == correct_word:
                                error_key = f"{incorrect_word}_{correct_word}_{dict_name}_{i}"
                                if error_key not in seen_errors:
                                    explanation = {
                                        "incorrect": incorrect_word,
                                        "correct": correct_word,
                                        "error_type": error_type,
                                        "description": f"غلط لفظ '{incorrect_word}' استعمال ہوا، صحیح لفظ '{correct_word}' ہونا چاہیے۔",
                                        "reason": reason
                                    }
                                    detected_errors.append(explanation)
                                    seen_errors.add(error_key)
                                    print(f"Detected phrase error: {explanation}")
                    else:
                        for correct_word, incorrect_word in error_dict.items():
                            pattern = r'^' + re.escape(incorrect_word) + r'([ںےکیکوسےمیںوں]*)$'
                            input_match = re.match(pattern, input_word)
                            if input_match:
                                suffix = input_match.group(1)
                                if corrected_word == correct_word + suffix:
                                    error_key = f"{input_word}_{corrected_word}_{dict_name}_{i}"
                                    if error_key not in seen_errors:
                                        explanation = {
                                            "incorrect": input_word,
                                            "correct": corrected_word,
                                            "error_type": error_type,
                                            "description": f"غلط لفظ '{input_word}' استعمال ہوا، صحیح لفظ '{corrected_word}' ہونا چاہیے۔",
                                            "reason": reason
                                        }
                                        detected_errors.append(explanation)
                                        seen_errors.add(error_key)
                                        print(f"Detected error: {explanation}")
        elif tag == 'delete':
            # Input word was omitted, check if it was incorrect
            for i in range(i1, i2):
                input_word = unicodedata.normalize('NFC', input_words[i])
                print(f"Checking deleted input[{i}]={input_word}")
                for dict_name, error_dict, error_type, reason in error_dictionaries:
                    for correct_word, incorrect_word in error_dict.items():
                        pattern = r'^' + re.escape(incorrect_word) + r'([ںےکیکوسےمیںوں]*)$'
                        input_match = re.match(pattern, input_word)
                        if input_match:
                            suffix = input_match.group(1)
                            expected_correct = correct_word + suffix
                            error_key = f"{input_word}_{expected_correct}_{dict_name}_{i}"
                            if error_key not in seen_errors:
                                explanation = {
                                    "incorrect": input_word,
                                    "correct": expected_correct,
                                    "error_type": error_type,
                                    "description": f"غلط لفظ '{input_word}' استعمال ہوا، صحیح لفظ '{expected_correct}' ہونا چاہیے۔",
                                    "reason": reason
                                }
                                detected_errors.append(explanation)
                                seen_errors.add(error_key)
                                print(f"Detected omitted word error: {explanation}")
        elif tag == 'insert':
            # Corrected text added a word, skip for error detection
            print(f"Inserted words at corrected[{j1}:{j2}]={corrected_words[j1:j2]}")

    return detected_errors

class SentenceInput(BaseModel):
    input_sentence: str

@app.post("/correct_sentence")
async def correct_sentence(input_data: SentenceInput):
    input_sentence = input_data.input_sentence
    
    # Tokenize and correct the sentence using the fine-tuned model
    prompt = "جملے کی درستگی: " + input_sentence
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=512,
            num_beams=5,
            early_stopping=True
        )
    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Detect errors by comparing input and corrected sentence
    errors = detect_errors(input_sentence, corrected_sentence)
    
    return {
        "input_sentence": input_sentence,
        "corrected_sentence": corrected_sentence,
        "errors": errors
    }


# import re
# import torch
# import unicodedata
# from fastapi import FastAPI
# from fastapi.middleware.cors import CORSMiddleware
# from pydantic import BaseModel
# from transformers import MT5Tokenizer, MT5ForConditionalGeneration
# from difflib import SequenceMatcher

# app = FastAPI()

# # Configure CORS
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["http://localhost:4200"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # Load the fine-tuned mT5 model and tokenizer
# model_path = "D:/FYP2/WebApplication/FinetunedModel/finetuned_mt5_large"
# tokenizer = MT5Tokenizer.from_pretrained(model_path, use_fast=False)
# model = MT5ForConditionalGeneration.from_pretrained(model_path)

# # Error dictionaries (same as provided)
# noun_errors = {
#     "پاکستان": "پاکیستان", "لوگ": "لوک", "مسائل": "مسایل", "مہنگائی": "مہنگای", "حکومت": "حوکمت",
#     "تعلیم": "تعلم", "صحت": "سحت", "گاؤں": "گاؤن", "موسم": "موسوم", "ٹیکنالوجی": "ٹکنالوجی",
#     "پانی": "پانے", "کرپشن": "کرفشن", "روزگار": "روزگاز", "سیاحت": "سایاحت", "نوجوان": "نووجوان",
#     "زراعت": "زرات", "صنعت": "سنعت", "سیلاب": "سیلاپ", "ٹرانسپورٹ": "ٹرانسپوٹ", "خواتین": "خواتیں",
#     "کھیل": "کحیل", "آلودگی": "الودگی", "امید": "امد", "دوستی": "دوسٹی", "زندگی": "زندکی"
# }

# verb_errors = {
#     "کرتے": "کرو", "بتا": "بولو", "بڑھ": "چڑھ", "ہو": "جا", "آتی": "جاتی", "رکھنا": "اٹھانا",
#     "سوچنا": "سونگھنا", "ملتی": "ٹلتی", "آزماتے": "چکھتے", "نکلتی": "پھسلتی", "دیتا": "بیچتا",
#     "بدل": "پھٹ", "رکھتی": "پکڑتی", "آتا": "بھاگتا", "چھوڑ": "پھوڑ", "لینا": "دھونا", "دیکھنا": "چومنا",
#     "سمجھنا": "چلنا", "پڑھنا": "گھومنا", "چلنا": "ہنسنا", "لگایا": "جلایا", "کمایا": "سمجھایا",
#     "کیا": "پیا", "دیا": "کیا", "کیں": "سیں", "آئیں": "جائیں", "دیں": "لیں", "کریں": "سریں"
# }

# pronoun_errors = {
#     "وہ": "یہ", "ہم": "تم", "اس": "ان", "ان": "اس", "ہمارے": "تمہارے", "یہ": "وہ", "کسی": "کوئی",
#     "جو": "جس", "ہر": "کوئی", "اپنی": "ان کی", "تم": "ہم", "میں": "ہم", "ہمیں": "تمہیں", "انہوں": "اس نے",
#     "کس": "کیا", "کچھ": "سب", "کوئی": "ہر", "خود": "دوسرے", "ان کا": "ہمارا", "اپنوں": "دوسروں",
#     "ہم سب": "تم سب", "ان سب": "اس سب", "کس نے": "کیا نے", "جس نے": "جو نے", "جو کچھ": "کچھ بھی",
#     "ہر ایک": "کوئی ایک", "ان کے": "ہمارے", "ہم نے": "تم نے", "تم نے": "ہم نے"
# }

# prepositions_conjunctions_errors = {
#     "میں": "سے", "سے": "میں", "کے": "پر", "پر": "کے", "اور": "یا", "تا کہ": "کیونکہ", "کی": "کا",
#     "کہ": "جو", "لیے": "بغیر", "بھی": "نہ", "جس": "کہ", "اگر": "ورنہ", "تو": "مگر", "جب": "کہاں",
#     "تک": "سے", "چونکہ": "تاکہ", "کیونکہ": "اگر", "جو": "کہ", "جن": "جس", "ورنہ": "اگر", "یا": "اور",
#     "مگر": "تو", "بل کہ": "بلکہ", "جب کہ": "کیونکہ"
# }

# object_errors = {
#     "مسائل": "مسایئل", "مہنگائی": "مہنگای", "تعلیم": "تلیم", "صحت": "سحت", "پانی": "پانے", "کرپشن": "کرپسشن",
#     "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان", "زراعت": "زراعٹ", "صنعت": "سنعت",
#     "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خواتین": "خواتیں", "کھیل": "خیل", "آلودگی": "آلدگی",
#     "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "اصلاحات": "اصلحات",
#     "سرمایہ": "سرمائہ", "منصوبے": "منصوبہ", "حقوق": "حقووق", "وسائل": "وسائیل"
# }

# past_tense_verb_errors = {
#     "بڑھ گئی": "بڑھتا", "ہو گیا": "ہوتا", "متاثر ہوئی": "متاثر ہوتا", "آ گئے": "آتا", "کہا": "کہتا",
#     "بتایا": "بتاتا", "بنائے گئے": "بناتا", "دیا": "دیتا", "پڑھا": "پڑھتا", "چلا": "چلتا", "لگا": "لگتا",
#     "ہوئے": "ہوتا", "رکھا": "رکھتا", "کیا": "کرتا", "آئی": "آتا", "دیکھا": "دیکھتا", "سمجھا": "سمجھتا",
#     "چھوڑا": "چھوڑتا", "ہوئیں": "ہوتا", "پہنچا": "پہنچتا", "مل گیا": "ملتا", "بدل گیا": "بدلتا",
#     "کمایا": "کماتا", "لگایا": "لگاتا", "بنا": "بناتا", "بڑھائی": "بڑھاتا", "پایا": "پاتا", "کیے": "کرتا",
#     "سوچا": "سوچتا", "آزمایا": "آزماتا", "نکلا": "نکلتا", "بدلا": "بدلتا", "آیا": "آتا", "لیا": "لیتا",
#     "رکھے": "رکھتا", "کیں": "کرتا", "آئیں": "آتا", "دیں": "دیتا", "بڑھائیں": "بڑھاتا", "پائیں": "پاتا",
#     "کریں": "کرتا", "ہوئی": "ہوتا", "گئی": "جاتا"
# }

# present_tense_verb_errors = {
#     "کرتے ہیں": "کریں گے", "بتا رہا ہے": "بتائے گا", "بڑھ رہی ہے": "بڑھے گی", "ہوتا ہے": "ہو گا",
#     "لگتا ہے": "لگے گا", "چاہیے": "چاہیے گا", "دینی ہے": "دے گا", "متاثر ہو رہا ہے": "متاثر ہو گا",
#     "پڑتا ہے": "پڑے گا", "حاصل کر رہے ہیں": "حاصل کریں گے", "دیتی ہے": "دے گی", "بناتے ہیں": "بنائیں گے",
#     "آتی ہے": "آئے گی", "رکھتا ہے": "رکھے گا", "سوچتے ہیں": "سوچیں گے", "ملتی ہے": "ملے گی",
#     "آزماتے ہیں": "آزمائیں گے", "نکلتی ہے": "نکلے گی", "دیتا ہے": "دے گا", "بدلتے ہیں": "بدلیں گے",
#     "رکھتی ہے": "رکھے گی", "آتا ہے": "آئے گا", "چھوڑتے ہیں": "چھوڑیں گے", "لیتے ہیں": "لیں گے",
#     "دیکھتے ہیں": "دیکھیں گے", "سمجھتے ہیں": "سمجھیں گے", "پڑھتے ہیں": "پڑھیں گے", "چلتے ہیں": "چلیں گے",
#     "لگاتے ہیں": "لگائیں گے", "کماتے ہیں": "کمائیں گے", "رکھتے ہیں": "رکھیں گے", "کرتی ہے": "کرے گی",
#     "بتاتی ہے": "بتائے گی", "بڑھتا ہے": "بڑھے گا", "لگتی ہے": "لگے گی", "دیتے ہیں": "دیں گے",
#     "بناتی ہے": "بنائے گی", "آتے ہیں": "آئیں گے", "رکھتی ہیں": "رکھیں گی", "سوچتی ہے": "سوچے گی",
#     "ملتا ہے": "ملے گا", "آزماتی ہے": "آزمائے گی", "نکلتے ہیں": "نکلیں گے", "بدلتی ہے": "بدلے گی",
#     "چھوڑتی ہے": "چھوڑے گی", "لیتی ہے": "لے گی", "دیکھتی ہے": "دیکھے گی", "سمجھتی ہے": "سمجھے گی",
#     "پڑھتی ہے": "پڑھے گی", "چلتی ہے": "چلے گی", "لگاتی ہے": "لگائے گی", "کماتی ہے": "کمائے گی",
#     "کر رہے ہیں": "کریں گے", "بتا رہے ہیں": "بتائیں گے", "بڑھ رہے ہیں": "بڑھیں گے", "لگ رہے ہیں": "لگیں گے"
# }

# future_tense_verb_errors = {
#     "ہو گا": "ہوا", "کر سکتے ہیں": "کر سکتے تھے", "آئے گا": "آیا", "دیں گے": "دیا", "بنا سکتے ہیں": "بنا سکتے تھے",
#     "پائیں گے": "پایا", "ملے گا": "ملا", "ہوں گی": "تھیں", "کریں گے": "کیا", "بڑھے گا": "بڑھا",
#     "رکھ سکتے ہیں": "رکھ سکتے تھے", "سوچ سکتے ہیں": "سوچ سکتے تھے", "بدل سکتے ہیں": "بدل سکتے تھے",
#     "چھوڑیں گے": "چھوڑا", "لیں گے": "لیا", "آئیں گے": "آئے", "پڑھ سکتے ہیں": "پڑھ سکتے تھے",
#     "دیکھیں گے": "دیکھا", "سمجھیں گے": "سمجھا", "ہو سکتے ہیں": "ہو سکتے تھے", "لگے گا": "لگا",
#     "کمایا جائے گا": "کمایا گیا", "لگایا جائے گا": "لگایا گیا", "بنایا جائے گا": "بنایا گیا",
#     "بڑھایا جائے گا": "بڑھایا گیا", "پایا جائے گا": "پایا گیا", "کیا جائے گا": "کیا گیا",
#     "رکھا جائے گا": "رکھا گیا", "دیا جائے گا": "دیا گیا"
# }

# singular_errors = {
#     "پاکستان": "پاکستن", "دوست": "دووست", "مسئلہ": "مسئیلہ", "مہنگائی": "مہنگای", "حکومت": "حکومٹ",
#     "تعلیم": "تلیم", "صحت": "سحت", "گاؤں": "گاؤن", "موسم": "موسسم", "ٹیکنالوجی": "ٹکنالوجی",
#     "پانی": "پانے", "کرپشن": "کرپسشن", "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان",
#     "زراعت": "زراعٹ", "صنعت": "سنعت", "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خاتون": "خاتوون",
#     "کھیل": "خیل", "آلودگی": "آلدگی", "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے"
# }

# plural_errors = {
#     "لوگ": "لوک", "مسائل": "مسایئل", "اخراجات": "اخرجت", "سہولتیں": "سہولتین", "ادارے": "ادارہ",
#     "طلبہ": "طلباء", "والدین": "والداین", "نوجوانوں": "نواجوانوں", "موسموں": "موسسموں", "مہارتیں": "مہارتین",
#     "وسائل": "وسائیل", "مواقع": "مواقق", "اصلاحات": "اصلحات", "منصوبوں": "منصوبہ", "حقوق": "حقووق",
#     "کھیلوں": "خیلوں", "شہروں": "شہرن", "بچوں": "بچن", "پروگرامز": "پروگرمز", "کاروباروں": "کارباروں",
#     "سیاحوں": "سیاحن", "مریضوں": "مریضن", "پالیسیوں": "پالسیوں", "اتفاقیات": "اتفقیات", "یادیں": "یادین"
# }

# masculine_errors = {
#     "دوست": "دووست", "شخص": "شخس", "نوجوان": "نواجوان", "طالب": "طلیب", "والد": "والدد", "کسان": "کسسن",
#     "سیاح": "سیح", "مریض": "مرض", "ڈاکٹر": "ڈکٹر", "سیاستدان": "سیاسدان", "گاؤں": "گاؤن", "شہر": "شہہر",
#     "موسم": "موسسم", "روزگار": "روزگارد", "کاروبار": "کاربار", "منصوبہ": "منصوبہہ", "کھیل": "خیل",
#     "پانی": "پانے", "ماحول": "محول", "وسائل": "وسائیل", "امن": "امم", "صبر": "سبر", "خواب": "خوب",
#     "رشتہ": "رشہ", "چیلنج": "چلنج"
# }

# feminine_errors = {
#     "مہنگائی": "مہنگای", "حکومت": "حکومٹ", "تعلیم": "تلیم", "صحت": "سحت", "ٹیکنالوجی": "ٹکنالوجی",
#     "کرپشن": "کرپسشن", "سیاحت": "سیاحٹ", "زراعت": "زراعٹ", "صنعت": "سنعت", "آلودگی": "آلدگی",
#     "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولت": "سہوللت", "اصلاح": "اصلح",
#     "سرمایہ": "سرمائہ", "پالیسی": "پالسی", "خاتون": "خاتوون", "مہارت": "محارت", "کوشش": "کوشس",
#     "مسکراہٹ": "مسکاہٹ", "خاموشی": "خموشی", "خوشی": "خشی", "یاد": "یادد", "ترقی": "ترقے"
# }

# direct_object_errors = {
#     "مسائل": "مسایئل", "مہنگائی": "مہنگای", "تعلیم": "تلیم", "صحت": "سحت", "پانی": "پانے", "کرپشن": "کرپسشن",
#     "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان", "زراعت": "زراعٹ", "صنعت": "سنعت",
#     "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خواتین": "خواتیں", "کھیل": "خیل", "آلودگی": "آلدگی",
#     "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "اصلاحات": "اصلحات",
#     "سرمایہ": "سرمائہ", "منصوبے": "منصوبہ", "حقوق": "حقووق", "وسائل": "وسائیل"
# }

# indirect_object_errors = {
#     "پاکستان میں": "پاکستن میں", "گاؤں میں": "گاؤن میں", "شہر میں": "شہہر میں", "مارکیٹ میں": "مارکیٹٹ میں",
#     "اسکولوں میں": "اسکولن میں", "اسپتالوں میں": "اسپتلن میں", "شعبوں میں": "شعبن میں", "میدانوں میں": "میدانن میں",
#     "اداروں میں": "ادارن میں", "منصوبوں میں": "منصوبن میں", "موسموں میں": "موسسموں میں", "حالات میں": "حالت میں",
#     "زندگی میں": "زندگے میں", "معاشرے میں": "معاشرہ میں", "دور میں": "دورر میں", "وقت میں": "وققت میں",
#     "ماحول میں": "محول میں", "سہولتوں میں": "سہولتین میں", "مسائل میں": "مسایئل میں", "حقوق میں": "حقووق میں",
#     "ترقی میں": "ترقے میں", "امن میں": "امم میں", "خوابوں میں": "خوابن میں", "امید میں": "امد میں",
#     "دوستی میں": "دوستے میں"
# }

# adverb_errors = {
#     "زیادہ": "زیادہہ", "فوری": "فورری", "سست": "سسٹ", "برابر": "برابار", "مناسب طور پر": "مناسب طورر پر",
#     "شدید": "شدیدد", "آہستہ": "آہستہہ", "ہمیشہ": "ہمشہ", "کبھی": "کبھے", "ابھی": "ابھے", "پہلے": "پہلہ",
#     "بعد": "بدد", "صرف": "سررف", "بہت": "بہہت", "کم": "ککم", "دوبارہ": "دوبارہہ", "اکثر": "اکسر",
#     "ہر سال": "ہرر سال", "جلدی": "جلدے", "صحیح": "سحیح", "واضح": "واضضح", "خاموشی سے": "خموشی سے",
#     "درست": "درسٹ", "سادہ": "سادہہ"
# }

# possessive_noun_errors = {
#     "پاکستان کی": "پاکستن کی", "لوگوں کی": "لوکوں کی", "حکومت کی": "حکومٹ کی", "تعلیم کی": "تلیم کی",
#     "صحت کی": "سحت کی", "گاؤں کی": "گاؤن کی", "موسم کی": "موسسم کی", "ٹیکنالوجی کی": "ٹکنالوجی کی",
#     "پانی کی": "پانے کی", "کرپشن کی": "کرپسشن کی", "روزگار کی": "روزگارد کی", "سیاحت کی": "سیاحٹ کی",
#     "نوجوانوں کی": "نواجوانوں کی", "زراعت کی": "زراعٹ کی", "صنعت کی": "سنعت کی", "سیلاب کی": "سیلب کی",
#     "ٹرانسپورٹ کی": "ٹرانسپرٹ کی", "خواتین کی": "خواتیں کی", "کھیل کی": "خیل کی", "آلودگی کی": "آلدگی کی",
#     "امید کی": "امد کی", "دوستی کی": "دوستے کی", "زندگی کی": "زندگے کی", "سہولت کی": "سہوللت کی",
#     "اصلاح کی": "اصلح کی"
# }

# possessed_noun_errors = {
#     "معیشت": "معشیت", "ترقی": "ترقے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "حالت": "حالتت",
#     "پیداوار": "پیدوار", "سرمایہ": "سرمائہ", "پالیسی": "پالسی", "مہارت": "محارت", "کوشش": "کوشس",
#     "مسکراہٹ": "مسکاہٹ", "خاموشی": "خموشی", "خوشی": "خشی", "یاد": "یادد", "امید": "امد", "دوستی": "دوستے",
#     "سادگی": "سادگے", "سکون": "سکوون", "احساس": "احسس", "شخصیت": "شخسیت", "بنیاد": "بنیادد",
#     "رفتاری": "رفتارے", "چیزوں": "چیزن", "اعتماد": "اعتمد"
# }

# sentence_component_errors = {
#     "پاکستان": "پاکستن", "لوگ": "لوک", "کرتے": "کریت", "مسائل": "مسایئل", "مہنگائی": "مہنگای",
#     "حکومت": "حکومٹ", "تعلیم": "تلیم", "صحت": "سحت", "گاؤں": "گاؤن", "موسم": "موسسم", "ٹیکنالوجی": "ٹکنالوجی",
#     "پانی": "پانے", "کرپشن": "کرپسشن", "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان",
#     "زراعت": "زراعٹ", "صنعت": "سنعت", "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خواتین": "خواتیں",
#     "کھیل": "خیل", "آلودگی": "آلدگی", "امید": "امد", "دوستی": "دوستے"
# }

# subject_errors = {
#     "لوگ": "لوک", "دوست": "دووست", "شخص": "شخس", "حکومت": "حکومٹ", "نوجوان": "نواجوان", "والدین": "والداین",
#     "کسان": "کسسن", "سیاح": "سیح", "مریض": "مرض", "ڈاکٹر": "ڈکٹر", "سیاستدان": "سیاسدان", "طلبہ": "طلباء",
#     "خواتین": "خواتیں", "پاکستان": "پاکستن", "گاؤں": "گاؤن", "شہر": "شہہر", "موسم": "موسسم",
#     "ٹیکنالوجی": "ٹکنالوجی", "پانی": "پانے", "کرپشن": "کرپسشن", "روزگار": "روزگارد", "سیاحت": "سیاحٹ",
#     "زراعت": "زراعٹ", "صنعت": "سنعت", "آلودگی": "آلدگی"
# }

# verb_object_errors = {
#     "مسائل": "مسایئل", "مہنگائی": "مہنگای", "تعلیم": "تلیم", "صحت": "سحت", "پانی": "پانے", "کرپشن": "کرپسشن",
#     "روزگار": "روزگارد", "سیاحت": "سیاحٹ", "نوجوان": "نواجوان", "زراعت": "زراعٹ", "صنعت": "سنعت",
#     "سیلاب": "سیلب", "ٹرانسپورٹ": "ٹرانسپرٹ", "خواتین": "خواتیں", "کھیل": "خیل", "آلودگی": "آلدگی",
#     "امید": "امد", "دوستی": "دوستے", "زندگی": "زندگے", "سہولتیں": "سہولتین", "اصلاحات": "اصلحات",
#     "سرمایہ": "سرمائہ", "منصوبے": "منصوبہ", "حقوق": "حقووق", "وسائل": "وسائیل"
# }

# # Combine all dictionaries into a list for sequential processing
# error_dictionaries = [
#     ("noun_errors", noun_errors, "اسم کی غلطی", "یہ اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
#     ("verb_errors", verb_errors, "فعل کی غلطی", "یہ فعل کی غلطی ہے کیونکہ یہ غلط زمانے یا شکل میں ہے۔"),
#     ("pronoun_errors", pronoun_errors, "ضمیر کی غلطی", "یہ ضمیر کی غلطی ہے کیونکہ یہ غلط فاعل یا مفعول کی طرف اشارہ کرتا ہے۔"),
#     ("prepositions_conjunctions_errors", prepositions_conjunctions_errors, "حرف ربط یا حرف جار کی غلطی", "یہ حرف ربط یا حرف جار کی غلطی ہے کیونکہ اسے جملے میں غلط استعمال کیا گیا ہے۔"),
#     ("object_errors", object_errors, "مفعول کی غلطی", "یہ مفعول کی غلطی ہے کیونکہ مفعول کی ہجے غلط ہے یا غلط ہے۔"),
#     ("past_tense_verb_errors", past_tense_verb_errors, "فعل ماضی کی غلطی", "یہ فعل ماضی کی غلطی ہے کیونکہ یہ غلط شکل میں ہے۔"),
#     ("present_tense_verb_errors", present_tense_verb_errors, "فعل حال کی غلطی", "یہ فعل حال کی غلطی ہے کیونکہ یہ غلط شکل میں ہے۔"),
#     ("future_tense_verb_errors", future_tense_verb_errors, "فعل مستقبل کی غلطی", "یہ فعل مستقبل کی غلطی ہے کیونکہ یہ غلط شکل میں ہے۔"),
#     ("singular_errors", singular_errors, "واحد کی غلطی", "یہ واحد اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
#     ("plural_errors", plural_errors, "جمع کی غلطی", "یہ جمع اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
#     ("masculine_errors", masculine_errors, "مذکر کی غلطی", "یہ مذکر اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
#     ("feminine_errors", feminine_errors, "مؤنث کی غلطی", "یہ مؤنث اسم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
#     ("direct_object_errors", direct_object_errors, "مفعول مستقیم کی غلطی", "یہ مفعول مستقیم کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
#     ("indirect_object_errors", indirect_object_errors, "مفعول غیر مستقیم کی غلطی", "یہ مفعول غیر مستقیم کی غلطی ہے کیونکہ اسے غلط استعمال کیا گیا ہے۔"),
#     ("adverb_errors", adverb_errors, "متعلق فعل کی غلطی", "یہ متعلق فعل کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا اسے غلط استعمال کیا گیا ہے۔"),
#     ("possessive_noun_errors", possessive_noun_errors, "اسم ملکیتی کی غلطی", "یہ اسم ملکیتی کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
#     ("possessed_noun_errors", possessed_noun_errors, "اسم مملوک کی غلطی", "یہ اسم مملوک کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
#     ("sentence_component_errors", sentence_component_errors, "جملہ کے جزو کی غلطی", "یہ جملہ کے جزو کی غلطی ہے کیونکہ اسے غلط استعمال کیا گیا ہے۔"),
#     ("subject_errors", subject_errors, "فاعل کی غلطی", "یہ فاعل کی غلطی ہے کیونکہ اس کی ہجے غلط ہے یا غلط ہے۔"),
#     ("verb_object_errors", verb_object_errors, "فعل اور مفعول کی غلطی", "یہ فعل اور مفعول کی غلطی ہے کیونکہ مفعول فعل کے ساتھ مطابقت نہیں رکھتا۔")
# ]



# def detect_errors(input_text, corrected_text):
#     """
#     Detect errors by comparing input text with corrected text using sequence alignment.
#     Identifies errors corrected by the model according to error dictionaries.
#     Returns a list of unique detected errors with explanations and reasons in Urdu.
#     """
#     detected_errors = []
#     seen_errors = set()  # To avoid duplicates

#     # Normalize Unicode to handle encoding variations
#     input_text = unicodedata.normalize('NFC', input_text)
#     corrected_text = unicodedata.normalize('NFC', corrected_text)

#     # Tokenize input and corrected text into words
#     input_words = re.findall(r'\S+', input_text)
#     corrected_words = re.findall(r'\S+', corrected_text)

#     # Log tokenized words for debugging
#     print("Input words:", input_words)
#     print("Corrected words:", corrected_words)

#     # Use SequenceMatcher to align input and corrected words
#     matcher = SequenceMatcher(None, input_words, corrected_words)
#     matches = matcher.get_opcodes()

#     # Log alignment operations
#     print("Alignment operations:", matches)

#     # Process alignment operations
#     for tag, i1, i2, j1, j2 in matches:
#         if tag == 'equal':
#             # Words match, no error
#             continue
#         elif tag == 'replace':
#             # Words differ, check for errors
#             for i in range(i1, i2):
#                 input_word = unicodedata.normalize('NFC', input_words[i])
#                 corrected_word = unicodedata.normalize('NFC', corrected_words[j1 + (i - i1)])
#                 print(f"Comparing replace at input[{i}]={input_word} -> corrected[{j1 + (i - i1)}]={corrected_word}")
#                 for dict_name, error_dict, error_type, reason in error_dictionaries:
#                     if dict_name in ["indirect_object_errors", "possessive_noun_errors", "adverb_errors"]:
#                         for correct_word, incorrect_word in error_dict.items():
#                             if input_word == incorrect_word and corrected_word == correct_word:
#                                 error_key = f"{incorrect_word}_{correct_word}_{dict_name}_{i}"
#                                 if error_key not in seen_errors:
#                                     explanation = {
#                                         "incorrect": incorrect_word,
#                                         "correct": correct_word,
#                                         "error_type": error_type,
#                                         "description": f"غلط لفظ '{incorrect_word}' استعمال ہوا، صحیح لفظ '{correct_word}' ہونا چاہیے۔",
#                                         "reason": reason
#                                     }
#                                     detected_errors.append(explanation)
#                                     seen_errors.add(error_key)
#                                     print(f"Detected phrase error: {explanation}")
#                     else:
#                         for correct_word, incorrect_word in error_dict.items():
#                             pattern = r'^' + re.escape(incorrect_word) + r'([ںےکیکوسےمیںوں]*)$'
#                             input_match = re.match(pattern, input_word)
#                             if input_match:
#                                 suffix = input_match.group(1)
#                                 if corrected_word == correct_word + suffix:
#                                     error_key = f"{input_word}_{corrected_word}_{dict_name}_{i}"
#                                     if error_key not in seen_errors:
#                                         explanation = {
#                                             "incorrect": input_word,
#                                             "correct": corrected_word,
#                                             "error_type": error_type,
#                                             "description": f"غلط لفظ '{input_word}' استعمال ہوا، صحیح لفظ '{corrected_word}' ہونا چاہیے۔",
#                                             "reason": reason
#                                         }
#                                         detected_errors.append(explanation)
#                                         seen_errors.add(error_key)
#                                         print(f"Detected error: {explanation}")
#         elif tag == 'delete':
#             # Input word was omitted, check if it was incorrect
#             for i in range(i1, i2):
#                 input_word = unicodedata.normalize('NFC', input_words[i])
#                 print(f"Checking deleted input[{i}]={input_word}")
#                 for dict_name, error_dict, error_type, reason in error_dictionaries:
#                     for correct_word, incorrect_word in error_dict.items():
#                         pattern = r'^' + re.escape(incorrect_word) + r'([ںےکیکوسےمیںوں]*)$'
#                         input_match = re.match(pattern, input_word)
#                         if input_match:
#                             suffix = input_match.group(1)
#                             expected_correct = correct_word + suffix
#                             error_key = f"{input_word}_{expected_correct}_{dict_name}_{i}"
#                             if error_key not in seen_errors:
#                                 explanation = {
#                                     "incorrect": input_word,
#                                     "correct": expected_correct,
#                                     "error_type": error_type,
#                                     "description": f"غلط لفظ '{input_word}' استعمال ہوا، صحیح لفظ '{expected_correct}' ہونا چاہیے۔",
#                                     "reason": reason
#                                 }
#                                 detected_errors.append(explanation)
#                                 seen_errors.add(error_key)
#                                 print(f"Detected omitted word error: {explanation}")
#         elif tag == 'insert':
#             # Corrected text added a word, skip for error detection
#             print(f"Inserted words at corrected[{j1}:{j2}]={corrected_words[j1:j2]}")

#     return detected_errors

# class SentenceInput(BaseModel):
#     input_sentence: str

# @app.post("/correct_sentence")
# async def correct_sentence(input_data: SentenceInput):
#     input_sentence = input_data.input_sentence
    
#     # Tokenize and correct the sentence using the fine-tuned model
#     inputs = tokenizer(input_sentence, return_tensors="pt", padding=True, truncation=True, max_length=512)
#     with torch.no_grad():
#         outputs = model.generate(
#             **inputs,
#             max_length=512,
#             num_beams=5,
#             early_stopping=True
#         )
#     corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
#     # Detect errors by comparing input and corrected sentence
#     errors = detect_errors(input_sentence, corrected_sentence)
    
#     return {
#         "input_sentence": input_sentence,
#         "corrected_sentence": corrected_sentence,
#         "errors": errors
#     }
# from transformers import MT5ForConditionalGeneration, MT5Tokenizer
# from fastapi import FastAPI
# from pydantic import BaseModel
# from fastapi.middleware.cors import CORSMiddleware
# app = FastAPI()
# # Add CORS middleware to allow requests from your Angular frontend
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # Or specify allowed domains (e.g., ["http://localhost:4200"])
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )
# # Load the model and tokenizer once at the startup
# model_path = "D:/FYP2/WebApplication/FinetunedModel/finetuned_mt5_large"
# model = MT5ForConditionalGeneration.from_pretrained(model_path)
# tokenizer = MT5Tokenizer.from_pretrained(model_path)

# # Define a Pydantic model to validate the input data
# class Sentence(BaseModel):
#     input_sentence: str

# # Define the POST endpoint to correct the sentence
# @app.post("/correct_sentence")
# async def correct_sentence(sentence: Sentence):


#     input_ids = tokenizer.encode( "جملے کی درستگی: " + sentence.input_sentence, return_tensors="pt", max_length=512, truncation=True)
#     output_ids = model.generate(input_ids, max_length=512, num_beams=1, num_return_sequences=1)
#     corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)
#     return {"corrected_sentence": corrected_sentence}

# # Run the application using uvicorn
# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)
    
    
# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel
# from fastapi.middleware.cors import CORSMiddleware
# from transformers import MT5ForConditionalGeneration, MT5Tokenizer
# import logging
# import re

# # Set up logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# app = FastAPI()

# # Add CORS middleware to allow requests from your Angular frontend
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # Or specify allowed domains (e.g., ["http://localhost:4200"])
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # Load the model and tokenizer once at startup
# model_path = "D:/FYP2/WebApplication/FinetunedModel/finetuned_mt5_large"
# try:
#     logger.info("Loading model and tokenizer...")
#     model = MT5ForConditionalGeneration.from_pretrained(model_path)
#     tokenizer = MT5Tokenizer.from_pretrained(model_path)
#     logger.info("Model and tokenizer loaded successfully.")
# except Exception as e:
#     logger.error(f"Failed to load model or tokenizer: {str(e)}")
#     raise Exception(f"Model loading failed: {str(e)}")

# # Define a Pydantic model to validate the input data
# class Sentence(BaseModel):
#     input_sentence: str

# # Function to clean extra_id tokens from output
# def clean_output(text: str) -> str:
#     """Remove <extra_id_0>, <extra_id_1>, etc., from the output."""
#     cleaned_text = re.sub(r'<extra_id_\d+>', '', text).strip()
#     return cleaned_text

# # Define the POST endpoint to correct the sentence
# @app.post("/correct_sentence")
# async def correct_sentence(sentence: Sentence):
#     try:
#         input_sentence = sentence.input_sentence
#         logger.info(f"Received input sentence: {input_sentence}")

#         # Tokenize input to get token length
#         input_tokens = tokenizer.encode(
#             input_sentence,
#             return_tensors="pt",
#             max_length=512,  # Fallback max_length to prevent excessive truncation
#             truncation=True
#         )
#         input_length = input_tokens.shape[1]  # Number of tokens
#         dynamic_max_length = input_length + 10  # Add buffer for corrections
#         logger.info(f"Input token length: {input_length}, Setting max_length: {dynamic_max_length}")

#         # Ensure max_length doesn’t exceed 512 (model’s maximum)
#         dynamic_max_length = min(dynamic_max_length, 512)

#         # Tokenize input again with dynamic max_length
#         input_ids = tokenizer.encode(
#             input_sentence,
#             return_tensors="pt",
#             max_length=dynamic_max_length,
#             truncation=True
#         )

#         # Generate corrected sentence
#         output_ids = model.generate(
#             input_ids,
#             max_length=dynamic_max_length,
#             num_beams=1,
#             num_return_sequences=1,
#             early_stopping=True
#         )

#         # Decode output
#         corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        
#         # Clean any residual <extra_id_0> or similar tokens
#         corrected_sentence = clean_output(corrected_sentence)
#         logger.info(f"Corrected sentence: {corrected_sentence}")

#         return {"corrected_sentence": corrected_sentence}

#     except Exception as e:
#         logger.error(f"Error processing sentence: {str(e)}")
#         raise HTTPException(status_code=500, detail=f"Error processing sentence: {str(e)}")

# # Run the application using uvicorn
# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)    
    
# from optimum.onnxruntime import ORTModelForConditionalGeneration
# from transformers import MT5Tokenizer
# from fastapi import FastAPI
# from pydantic import BaseModel
# from fastapi.middleware.cors import CORSMiddleware

# app = FastAPI()

# # Add CORS middleware to allow requests from your Angular frontend
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],  # Or specify allowed domains (e.g., ["http://localhost:4200"])
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # In-memory cache for repeated sentences
# cache = {}

# # Load the model and tokenizer
# model_path = "D:/FYP2/WebApplication/FinetunedModel/finetuned_mt5_large"
# onnx_model_path = "D:/FYP2/WebApplication/FinetunedModel/onnx_mt5_large"

# """
# # One-time export to ONNX with quantization (run this separately before starting the app)
# # This converts the model to ONNX format and applies INT8 quantization
# from optimum.onnxruntime import ORTModelForConditionalGeneration
# model = ORTModelForConditionalGeneration.from_pretrained(model_path, export=True)
# model.save_pretrained(onnx_model_path)
# # After exporting, the model is saved to onnx_model_path and can be loaded directly
# """

# # Load the quantized ONNX model and tokenizer
# model = ORTModelForConditionalGeneration.from_pretrained(onnx_model_path, provider="CPUExecutionProvider")
# tokenizer = MT5Tokenizer.from_pretrained(model_path)

# # Define a Pydantic model to validate the input data
# class Sentence(BaseModel):
#     input_sentence: str

# # Define the POST endpoint to correct the sentence
# @app.post("/correct_sentence")
# async def correct_sentence(sentence: Sentence):
#     input_sentence = sentence.input_sentence
    
#     # Check cache for repeated sentence
#     if input_sentence in cache:
#         return {"corrected_sentence": cache[input_sentence]}
    
#     # Tokenize input
#     input_ids = tokenizer.encode(
#         input_sentence,
#         return_tensors="pt",
#         max_length=512,
#         truncation=True
#     )
    
#     # Generate corrected sentence
#     output_ids = model.generate(
#         input_ids,
#         max_length=512,
#         num_beams=1,
#         num_return_sequences=1,
#         early_stopping=True
#     )
    
#     # Decode output
#     corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
#     # Store in cache
#     cache[input_sentence] = corrected_sentence
    
#     return {"corrected_sentence": corrected_sentence}

# # Run the application using uvicorn
# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="0.0.0.0", port=8000)    
    
    
    
    
    
    
    
    
    
    
    
    
# import torch
# from transformers import MT5ForConditionalGeneration, MT5Tokenizer
# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel
# from fastapi.middleware.cors import CORSMiddleware
# import uvicorn
# import logging
# import asyncio
# import time
# from contextlib import asynccontextmanager

# # Configure logging
# logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
# logger = logging.getLogger(__name__)

# app = FastAPI(title="MT5 Sentence Correction API")

# # Add CORS middleware
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # Set device
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# logger.info(f"Using device: {device}")

# # Model paths
# MODEL_PATH = "D:/FYP2/WebApplication/FinetunedModel/finetuned_mt5_large"

# # Global model and tokenizer
# model = None
# tokenizer = None

# class Sentence(BaseModel):
#     input_sentence: str

# @asynccontextmanager
# async def timeout(seconds):
#     """Context manager for setting a timeout."""
#     start_time = time.time()
#     yield
#     if time.time() - start_time > seconds:
#         raise TimeoutError("Operation timed out")

# async def load_model():
#     """Load model and tokenizer with optimizations."""
#     global model, tokenizer
#     try:
#         logger.info("Loading tokenizer...")
#         tokenizer = MT5Tokenizer.from_pretrained(MODEL_PATH)
#         logger.info("Loading model...")
#         model = MT5ForConditionalGeneration.from_pretrained(MODEL_PATH)
#         model = model.to(device).half()  # Use FP16
#         model.gradient_checkpointing_enable()  # Reduce memory usage
#         model.eval()
#         logger.info("Model and tokenizer loaded successfully")
#     except Exception as e:
#         logger.error(f"Failed to load model: {str(e)}")
#         raise

# @app.on_event("startup")
# async def startup_event():
#     """Load model on startup."""
#     try:
#         await load_model()
#     except Exception as e:
#         logger.error(f"Startup failed: {str(e)}")
#         raise

# @app.post("/correct_sentence")
# async def correct_sentence(sentence: Sentence):
#     """Correct input sentence using MT5 model."""
#     try:
#         # Validate input
#         input_text = sentence.input_sentence.strip()
#         if not input_text:
#             raise HTTPException(status_code=400, detail="Input sentence cannot be empty")
#         if len(input_text) > 1000:
#             raise HTTPException(status_code=400, detail="Input sentence too long (max 1000 characters)")

#         logger.info(f"Processing input: {input_text[:50]}...")

#         # Encode input
#         inputs = tokenizer(
#             input_text,
#             return_tensors="pt",
#             max_length=128,  # Further reduced for speed
#             truncation=True,
#             padding=True
#         ).to(device)

#         # Generate correction with timeout
#         async with timeout(10):  # 10-second timeout
#             with torch.inference_mode():
#                 logger.info("Generating correction...")
#                 output_ids = model.generate(
#                     input_ids=inputs["input_ids"],
#                     attention_mask=inputs["attention_mask"],
#                     max_length=128,
#                     num_beams=1,  # Minimal beams for speed
#                     early_stopping=True,
#                     num_return_sequences=1
#                 )

#         corrected_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)
#         logger.info("Correction generated successfully")
#         return {"corrected_sentence": corrected_sentence}

#     except TimeoutError:
#         logger.error("Inference timed out")
#         raise HTTPException(status_code=504, detail="Model inference timed out")
#     except Exception as e:
#         logger.error(f"Error processing request: {str(e)}")
#         raise HTTPException(status_code=500, detail=f"Error correcting sentence: {str(e)}")

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000)